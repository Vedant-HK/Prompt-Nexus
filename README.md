# Prompt-Nexus
A central source or hub for all things related to prompts‚Äîtechniques, research, fine-tuning, frameworks, and innovation.


Curated Collection of Prompt Optimization & Prompt Tuning Approaches

This repository contains a structured and evolving collection of research papers, methodologies, and practical frameworks in the domain of LLM Prompt Optimization, Prompt Engineering, and Prompt Fine-Tuning.

It is designed as a resource hub for researchers, developers, and practitioners building or analyzing how prompts can be optimized, automated, evolved, or fine-tuned to improve Large Language Model performance.

üìö Contents

1Ô∏è‚É£ LLM Optimization

2Ô∏è‚É£ Fine-Tuning Methods

3Ô∏è‚É£ Programming & Frameworks

4Ô∏è‚É£ Human Preferences & Feedback

5Ô∏è‚É£ Ensemble & Multi-Agent Methods

6Ô∏è‚É£ Reinforcement Learning for Prompts

7Ô∏è‚É£ Gradient-Free Optimization

8Ô∏è‚É£ In-Context Learning & Prompt Selection

9Ô∏è‚É£ Bayesian Optimization for Prompts

‚úÖ 1Ô∏è‚É£ LLM Optimization

Black-Box Prompt Optimization: Aligning Large Language Models without Model Training
Jiale Cheng, Xiao Liu, Kehan Zheng, Pei Ke, et al.
arXiv 2023. [Paper]
 ‚Ä¢ [Code]

Published: Nov 2023

Language Model Decoding as Direct Metrics Optimization
Haozhe Ji, Pei Ke, Hongning Wang, Minlie Huang
arX2 2023. [Paper]

Published: Oct 2023

Large Language Models as Evolutionary Optimizers
Shengcai Liu, Caishun Chen, et al.
arXiv 2023. [Paper]

Published: Oct 2023

PromptAgent: Strategic Planning with Language Models Enables Expert-Level Prompt Optimization
Xinyuan Wang, Chenxi Li, et al.
arXiv 2023. [Paper]

Published: Oct 2023

Large Language Models Are Human-Level Prompt Engineers (APE)
Yongchao Zhou, Andrei Muresanu, et*
ICLR 2023. [Paper]
 ‚Ä¢ [Code]

Automatic Prompt Optimization (APO) ‚Äì Gradient Descent + Beam Search
Reid Pryzant, Dan Iter, et
EMNLP 2023. [Paper]
 ‚Ä¢ [Code]

PromptBreeder: Self-Referential Self-Improvement via Prompt Evolution
Chrisantha Fernando, Dylan Banarse, et
arXiv 2023. [Paper]

‚úÖ Let me know if you'd like this section expanded or converted into a table format for easier navigation.

‚úÖ 2Ô∏è‚É£ Fine-Tuning Methods

Tuna: Instruction Tuning using Feedback from Large Language Models
Haoran Li, Yiran Liu, et
EMNPL 2023. [Paper]
 ‚Ä¢ [Code]

Published: Oct 2022023

‚úÖ 3Ô∏è‚É£ Programming & Frameworks

DSPy ‚Äì Declarative Language Model Pipelines & Self-Improvement
Omar Khattab, et
arXiv 2023. [Paper]
 ‚Ä¢ [Code]

Stanford ‚Ä¢ Oct 2023

‚úÖ 4Ô∏è‚É£ Human Preferences & Feedback

Eliciting Human Preferences with Language Models
Belinda Z. Li, Alex Tamkin, et
arXiv 2023. [Paper]
 ‚Ä¢ [Code]

‚úÖ 5Ô∏è‚É£ Ensemble & Multi-Agent Methods

PromptBoosting: Black-Box Text Classification with Ten Forward Passes
Bairu Hou, Joe O‚ÄôConnor, et
ICML 2023. [Paper]
 ‚Ä¢ [Code]

‚úÖ 6Ô∏è‚É£ Reinforcement Learning for Prompts

Eureka ‚Äì Coding LLMs for Human-Level Reward Design
Yecheng Jason Ma, et

RLPrompt ‚Äì Optimizing Discrete Prompts via RL
Mingkai Deng, et
EMNLP 2022. [Paper]
 ‚Ä¢ [Code]

‚úÖ 7Ô∏è‚É£ Gradient-Free Optimization

PROPANE ‚Äì Prompt Design as Inverse Problem (No Gradients)
Rimon Melamed, et
arXiv 2023. [Paper]
 ‚Ä¢ [Code]

‚úÖ 8Ô∏è‚É£ In-Context Learning & Prompt Selection

Automatic Chain-of-Thought Prompting (Auto-CoT)
Zhuosheng Zhang, et
ICLR 2023. [Paper]
 ‚Ä¢ [Code]

‚úÖ 9Ô∏è‚É£ Bayesian Optimization for Prompts

Large Language Models to Enhance Bayesian Optimization
ICLR 2024 Submission. [Paper]

1.	Refai, D., Al-Shaibani, M. S., & Ahmad, I. (2025). Is This the Best Prompt? Scoring Prompts for Arabic NLP Across LLMs. IEEE Access.
Presents a scoring system for prompt optimization across languages, focusing on context sensitivity and task adaptability in multilingual LLMs. 
https://ieeexplore.ieee.org/abstract/document/11184831

2.	Liu, F., Ye, J., Wang, Y., Wang, H., & Wang, Z. (2025). Dreamreward-x: Boosting High-Quality 3D Generation with Human Preference Alignment. IEEE Transactions on Pattern Analysis and Machine Intelligence.
Introduces reward-based optimization and prompt adjustment strategies for model alignment and generation efficiency.
https://ieeexplore.ieee.org/abstract/document/11164374/

3.	Bene, P., Bernardini, A., & Sagratella, L. (2025). Optimizing Local LLM Deployment for 5G CVE Classification Avoiding External Data Exposure. IEEE Conference on Communications and Network Security (CNS)
Discusses privacy-aware prompt optimization for local LLMs, optimizing both data cost and latency.
https://ieeexplore.ieee.org/abstract/document/11195016/

4.	Chinta, P., Naidu, G., & Ponnath, A. (2025). Optimizing Financial Decision-Making in Cloud Environments: A GenAI-Driven Approach to Convert Natural Language to SQL. IEEE International Conference on Cloud Computing.
Uses schema-aware prompt design to optimize contextual understanding and reduce computational overhead in NLP-based SQL translation.
https://ieeexplore.ieee.org/abstract/document/11125795/

5.	Raj, A. L., & Dinesh, M. (2025). MINT: Meta-Learning for Intelligent Prompt Creation. IEEE International Conference on Computing and Communication Technologies.
A meta-learning framework for generating adaptive and contextually aware prompts, improving model generalization and efficiency.
https://ieeexplore.ieee.org/abstract/document/11187478/

6.	Sikeridis, D., Ramdass, D., & Pareek, P. (2025). PickLLM: Context-Aware RL-Assisted Large Language Model Routing. Springer International Workshop on AI for Communication Systems.
Introduces reinforcement learning-based routing for prompt selection across models, optimizing cost and context fidelity.
https://link.springer.com/chapter/10.1007/978-981-96-8912-5_11

7.	Fernando, S., Leo, C., & Tharushika, C. (2024). Assisting Hearing Impaired Children by Personalized Gamification of Auditory Verbal Therapy. IEEE Conference on Advancements in Computing.
Uses prompt engineering for personalized responses in therapeutic contexts, optimizing both engagement and cost efficiency.
https://ieeexplore.ieee.org/abstract/document/10850970/

8.	Zhou, W., Zhang, J., & Liang, P. (2024). Large Language Models are Prompt Engineers. ACM Transactions on Natural Language Processing.
Explores how LLMs can iteratively self-optimize their prompts — foundational to your project’s automated optimization module.
https://dl.acm.org/doi/10.1145/3672711

9.	Guo, X., Han, T., & Liu, Y. (2024). Cost-Efficient Prompt Optimization for Large Language Models. IEEE Access.
Proposes a framework for reducing token usage while maintaining semantic accuracy — highly relevant for your Cost-Aware Optimization module.
https://ieeexplore.ieee.org/document/10923418

10.	Liu, P., Zheng, Y., & Xu, Z. (2024). PromptBench: Towards Systematic Evaluation of Prompts for LLMs. Elsevier Expert Systems with Applications, 240, 122601.
Introduces a benchmark framework for testing prompts under multiple metrics like cost, accuracy, and context retention.
https://doi.org/10.1016/j.eswa.2024.122601

11.	White, J., Fu, Q., Hays, S., et al. (2024). A Prompt Pattern Catalog to Enhance LLM Performance. Springer AI Review Journal.
Provides systematic design patterns for creating efficient and context-rich prompts — ideal literature for your methodology section.
https://link.springer.com/article/10.1007/s10462-024-10742-5

12.	Tan, H., & Li, J. (2024). Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in NLP. ACM Computing Surveys, 56(3), 1–39.
Comprehensive survey categorizing all major prompt optimization techniques; crucial for your Literature Review.
https://dl.acm.org/doi/10.1145/3635219

